<HTML>
<HEAD>
<TITLE>Com S 472/572 - Final Exam</TITLE>
<link rel="stylesheet" href="homepage.css" type="text/css">
</HEAD>

<body bgcolor="white" vlink="red">

<center>
<font size="+2" color="green" face="helvetica"><b><i>Guide for the Final Exam</i></b></font>
<br>
<font size="+1" color="green" face="helvetica">
<b>Com S 472/572</b> <br>
<b> Fall 2020</b> </font>
</center>
<br><br>

<hr color="olive">

<font face="helvetica">


The final exam will be given out on Canvas for 
the 5-hour period of <font color="red"><b>8:15am - 1:15pm</b></font> on <font color="red"><b>Wednesday 
November 25th</b></font>.  This period subsumes the univerity scheduled time of 9:45-11:45am but extends both backward 
and forward by 90 minutes, in part to accommodate for lunch.  
Nevertheless, the exam will be designed to take about the 
<b>same amount of effort</b> that it would require if 
it were to be normally taken in 120 minutes on campus. 
<ul>
<li> On Wednesday November 25th at 8:15am, you will retrieve the final exam on Canvas under <tt>
Assignments -> Final Exam</tt>.
<li> The exam file will be out in two formats: .pdf and .pptx.     
<ul> 
<li> You may work on the .pptx file directly, typing in your answers and drawing figures.
<li> Submission can be made in either .pdf or .pptx format. 
<li> If using PowerPoint, you do not need to typeset math symbols and equations within the Equation environment. 
Just plain text with no font preference. For example, <tt>x + 2y = 3</tt>.  Also, don't bother with subscripts: 
A variable <tt>a</tt> with a subscript <tt>1</tt>
 can be simply typeset as <tt>a1</tt> as long as there's no ambiguity.  
Similarly, <tt>n log n</tt> would be interpreted as the 
product of <tt>n</tt> with its logarithm. 
<li>  Or, you may write up an answer by hand if it involves equations and math symbols, 
and then simply insert a picture
of your answer into PowerPoint. 
</ul>  
<li> You may print out a copy and write down your answers by hand. 
Pay attention to the following if you do so:
<ul> 
<li> Scan your answer sheets and submit them in a .pdf file.
<li> Be sure to scan the <font color="red"><b>first page</b></font> which shows your name and a scoring table.  
<li> You need only scan the remaining pages that show your answers.  
<li> You may also choose to write down all your answers on separate blank sheets for scanning.  
(Check that the problem numbers are correct for your answers.) 
<li> Please double check to make sure you haven't omitted any page with your writing before submission. 
</ul> 
<li> Submission must be done <font color="red"><b>before 1:15pm</b></font>, after which the submission link will disappear.
<font color="blue">Do <b>not</b> expect the instructor or TAs to accept your submission by e-mail past 1:15pm.</font> 
<li> Multiple submissions will be allowed but only the <b>latest version</b> will be graded. 
</ul>
In the case that the exam time does not work for you due to a conflict or some other issue, please contact 
the instructor (jia@iastate.edu)  
<font color="red"><b>no later than Wednesday November 18th</b></font>.  
<p> 

The following rules apply:
<ul>
<li><b><font color="red">closed-book</font></b> and <b><font color="red">closed-notes</font></b> during the exam. 
<li><b><font color="red">No online search</font></b> for information during the exam.  
<li><b><font color="red">No discussion or sharing in any form</font></b>  with others 
during or after the exam.
</ul>
<font color="blue">We trust your integrity, as we all 
understand that any violation by exploiting the situation with no proctoring would be such a <b>shame</b>. </font>

<p> 

According to the syllabus, the exam is weighted 30% (for Com S 472) and 27% (for Com S 572) of the overall grade.
It is a comprehensive exam with at least 5/6 of the weight assigned to the materials learned since the midterm exam. 
The exam will focus on the topics below: 
<ul>

<li><b>Propositional logic</b>: Horn clause, definite clause, fact, goal clause, logic programming, forward chaining, 
backward chaining, propositional model checking, DPLL algorithm, and local search. 

<p> 

<li><b>First-order logic</b>: 
<ul>
<li> syntax, constants, variables, functions, predicates, terms, quantifiers, 
nested quantification, quantifiers with precedence and negation, free and bound variables, and equality;
<li> model for FOL, assertions, queries, axioms, 
and theorems;
<li> domains of kinship and natural numbers, syntactic sugar, sets, wumpus world, and electronic circuits in FOL;
<li>instantiations of quantifiers, ground term, Skolem constant, generalized modus ponens, and unification;
<li> first-order definite clauses, translation of natural language sentences, simple forward chaining (FC), proof tree, 
matching a definite clause, and incremental FC;
<li> logic programming, backward chaining, depth-first proof tree, query and recursive matching in Prolog;  
<li> conversion to CNF, eliminations of implication, negation, standardization, Skolemization, and 
distribution of disjunction over conjuction;
<li> resolution in FOL, inference rule (with unification), proof examples, completeness, and handling of equality.
</ul>

<p> 

<li><b>Planning</b>: classical planning, PDDL, iniital state, goal state, action schema, add and delete lists, 
air cargo transport, the spare tire problem, the blocks world, forward state-space search, 
combinatorial explosion of state space, backward search, regression, propositionalization of planning, 
heuristics derivations, belief state, percept schema, action schema, sensorless planning, 
updating the belief state, and contingent planning.

<p> 

<li><b>Uncertain knowledge and reasoning</b>:
<ul>
<li> probability space, sample space, event, prior and posterior, random variable, cumulative distribution function, 
probability density function, uniform distribution, joint probability, Kolmogorov's axioms, and inconsistent beliefs;

<li> mean, variance, standard deviation, and Gaussian distribution; 
<li> joint distribution table, marginalization, 
conditioning, computation of conditional probabilities, normalization, and independent random variables; 
<li> Bayes' rule, causal and diagnostic relationships, omission of the evidence's prior probability, 
general form, conditional independence, decomposition of joint distribution, naive Bayes model, and text classification;
<li> Bayesian networks, DAG representation, conditional probability tables, semantics, probability calculation, 
domain representation, topological order, construction, condition for edge addition, effects of a node order,  
compactness, Markov blanket, and d-separation;
<li> posterior probability query, inference by enumeration, expression tree, summing out hidden variables, 
pointwise product, variable ordering, and handling of irrelevant variables; 
<li> direct sampling, approximate inference, event generation from random sampling, probability estimation for partially specified event, 
rejection sampling, importance sampling, and likelihood weighting; 
<li> Gibbs sampling, Markov blanket distribution, Markov chain, transition kernel, stationary distribution, 
detailed balance, correctness,  transition kernel and stationay distribution, metriopolis-hastings sampling, its transition kernel, 
proposal distribution, acceptance probability, and convergence;
<li> temporal models, state sequence, evidence sequence, Markov process, transitoin model, sensor, model, 
complete joint distribution, inference tasks, filtering, projection vs update, recursive estimation, prediction, 
mixing time, and smoothing;

<li> optimal state sequence, graph representation of possible state transitions, optimal path, Viterbi algorithm, 
hidden Markov models, matrix representations of transition and sensor models, time and space complexities, 
robot localization, characterization of a faulty sensor, and localization and path errors.

</ul>

<p>

<li><b>Machine learning:</b>
<ul>
<li> learning agents, applications of ML, supervised learning, training set, hypothesis space, ground truth, 
best-fit function, underfitting, overfitting, variance, and most probable hypothesis;
<li> decision trees, their expressiveness, poor and good attributes, construction algorithm, four cases, 
learning curve, entropy of a random variable, attribute selection, expected entropy after an attribute test, 
information gain, generalization, and applicability.
</ul>

</ul>



</font>

</body>
</html>












